{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(y, x, create_graph=False):                                                               \n",
    "    jac = []                                                                                          \n",
    "    flat_y = y.reshape(-1)                                                                            \n",
    "    grad_y = torch.zeros_like(flat_y)                                                                 \n",
    "    for i in range(len(flat_y)):                                                                      \n",
    "        grad_y[i] = 1.                                                                                \n",
    "        grad_x, = torch.autograd.grad(flat_y, x, grad_y, retain_graph=True, create_graph=create_graph)\n",
    "        jac.append(grad_x.reshape(x.shape))                                                           \n",
    "        grad_y[i] = 0.                                                                                \n",
    "    return torch.stack(jac).reshape(y.shape + x.shape)                                                \n",
    "                                                                                                      \n",
    "def hessian(y, x):                                                                                    \n",
    "    return jacobian(jacobian(y, x, create_graph=True), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.6931471824645996\n",
      "hessnorm 0.5\n",
      "loss 0.4740769863128662\n",
      "hessnorm 0.47000741958618164\n",
      "loss 0.34769776463508606\n",
      "hessnorm 0.4148704409599304\n",
      "loss 0.27001631259918213\n",
      "hessnorm 0.36127567291259766\n",
      "loss 0.21886718273162842\n",
      "hessnorm 0.31586238741874695\n",
      "loss 0.18315738439559937\n",
      "hessnorm 0.2787052094936371\n",
      "loss 0.15702718496322632\n",
      "hessnorm 0.24840296804904938\n",
      "loss 0.137176513671875\n",
      "hessnorm 0.2235051691532135\n",
      "loss 0.121634840965271\n",
      "hessnorm 0.20282311737537384\n",
      "loss 0.10916388034820557\n",
      "hessnorm 0.18544301390647888\n",
      "loss 0.09895133972167969\n",
      "hessnorm 0.1706741899251938\n",
      "loss 0.09044444561004639\n",
      "hessnorm 0.15799395740032196\n",
      "loss 0.08325493335723877\n",
      "hessnorm 0.14700418710708618\n",
      "loss 0.0771028995513916\n",
      "hessnorm 0.13739794492721558\n",
      "loss 0.07178163528442383\n",
      "hessnorm 0.12893632054328918\n",
      "loss 0.06713545322418213\n",
      "hessnorm 0.12143083661794662\n",
      "loss 0.06304502487182617\n",
      "hessnorm 0.11473149806261063\n",
      "loss 0.059417128562927246\n",
      "hessnorm 0.10871735215187073\n",
      "loss 0.05617833137512207\n",
      "hessnorm 0.10329021513462067\n",
      "loss 0.05326986312866211\n",
      "hessnorm 0.09836947172880173\n",
      "loss 0.0506439208984375\n",
      "hessnorm 0.09388837218284607\n",
      "loss 0.04826164245605469\n",
      "hessnorm 0.08979130536317825\n",
      "loss 0.04609084129333496\n",
      "hessnorm 0.08603152632713318\n",
      "loss 0.044104933738708496\n",
      "hessnorm 0.08256957679986954\n",
      "loss 0.042281150817871094\n",
      "hessnorm 0.07937169820070267\n",
      "loss 0.04060077667236328\n",
      "hessnorm 0.07640916854143143\n",
      "loss 0.0390477180480957\n",
      "hessnorm 0.07365722954273224\n",
      "loss 0.037607789039611816\n",
      "hessnorm 0.07109422981739044\n",
      "loss 0.036269426345825195\n",
      "hessnorm 0.06870166212320328\n",
      "loss 0.03502225875854492\n",
      "hessnorm 0.06646320968866348\n",
      "loss 0.03385722637176514\n",
      "hessnorm 0.06436452269554138\n",
      "loss 0.03276658058166504\n",
      "hessnorm 0.062392983585596085\n",
      "loss 0.031743526458740234\n",
      "hessnorm 0.06053752824664116\n",
      "loss 0.030781984329223633\n",
      "hessnorm 0.058788251131772995\n",
      "loss 0.0298764705657959\n",
      "hessnorm 0.05713631957769394\n",
      "loss 0.02902233600616455\n",
      "hessnorm 0.05557391420006752\n",
      "loss 0.02821528911590576\n",
      "hessnorm 0.054093942046165466\n",
      "loss 0.027451634407043457\n",
      "hessnorm 0.05269015580415726\n",
      "loss 0.026728034019470215\n",
      "hessnorm 0.05135682225227356\n",
      "loss 0.026041388511657715\n",
      "hessnorm 0.050088830292224884\n",
      "loss 0.025388717651367188\n",
      "hessnorm 0.048881400376558304\n",
      "loss 0.02476799488067627\n",
      "hessnorm 0.04773049056529999\n",
      "loss 0.024176597595214844\n",
      "hessnorm 0.04663211852312088\n",
      "loss 0.023612618446350098\n",
      "hessnorm 0.04558280110359192\n",
      "loss 0.02307415008544922\n",
      "hessnorm 0.04457937180995941\n",
      "loss 0.022559642791748047\n",
      "hessnorm 0.043618906289339066\n",
      "loss 0.02206730842590332\n",
      "hessnorm 0.04269862920045853\n",
      "loss 0.021596074104309082\n",
      "hessnorm 0.04181622341275215\n",
      "loss 0.021144390106201172\n",
      "hessnorm 0.04096932336688042\n",
      "loss 0.020711064338684082\n",
      "hessnorm 0.0401558019220829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yaroslavvb/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "x = torch.tensor([[0, 0.]], requires_grad=True)                                                                 \n",
    "target = torch.tensor([0])\n",
    "for i in range(50):\n",
    "    loss = F.nll_loss(F.log_softmax(x), target)\n",
    "    flat_x = x.reshape(-1)\n",
    "    grad = torch.autograd.grad(loss, x, retain_graph=True)[0]\n",
    "    hess = hessian(loss, x)\n",
    "    x = x - .5 * grad\n",
    "\n",
    "    print(\"loss\", loss.item())\n",
    "    print(\"hessnorm\", torch.norm(hess).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
